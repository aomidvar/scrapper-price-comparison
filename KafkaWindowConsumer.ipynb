{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP810JrqW3bBvrhV4WlpDwG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aomidvar/scrapper-price-comparison/blob/main/KafkaWindowConsumer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "An__vZPywreL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import urllib\n",
        "\n",
        "checkpoint_dir = \"/content/checkpoint\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'"
      ],
      "metadata": {
        "id": "cBak0shoxNuc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Spark version to install\n",
        "spark_version = \"3.2.4\"\n",
        "\n",
        "# Download Spark\n",
        "filename = f\"spark-{spark_version}-bin-hadoop3.2.tgz\"\n",
        "url = f\"https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop2.7.tgz\"\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# Extract Spark\n",
        "!tar -xvf $filename"
      ],
      "metadata": {
        "id": "vWi2XcfZw-37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install confluent-kafka[avro,json,protobuf]"
      ],
      "metadata": {
        "id": "qlxKJErPxGDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install confluent-kafka configparser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of3n2i2fxh1i",
        "outputId": "2fd6862b-1507-48af-abdd-97cc0d62ca07"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: confluent-kafka in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Collecting configparser\n",
            "  Downloading configparser-5.3.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: configparser\n",
            "Successfully installed configparser-5.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "configparser"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "iPQ_OysvxXnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pyarrow"
      ],
      "metadata": {
        "id": "ZCMRWZnaxaZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install avro-python3"
      ],
      "metadata": {
        "id": "pvXy_Nl5xcj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtw7kHydyflY",
        "outputId": "1f043f4d-47b4-4d19-ba38-04276599229c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark kafka-python"
      ],
      "metadata": {
        "id": "IOTHVbEoxela"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo[srv]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DeJNS3qxwkZ",
        "outputId": "af491dd3-4ef2-413c-f5b6-c4597e73c53a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymongo[srv]\n",
            "  Downloading pymongo-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0 (from pymongo[srv])\n",
            "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.3.0 pymongo-4.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kafka import KafkaConsumer, KafkaProducer\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from confluent_kafka import Consumer, KafkaException, KafkaError\n",
        "from pyspark.sql.functions import col, from_json, window\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DoubleType, TimestampType\n",
        "#from confluent_kafka.avro import AvroDeserializer\n",
        "from confluent_kafka.avro.serializer import SerializerError\n",
        "import json\n",
        "import avro.schema\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from confluent_kafka import Consumer, KafkaException, KafkaError\n",
        "from datetime import datetime, timedelta\n"
      ],
      "metadata": {
        "id": "s9KTpJewxp_C"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kafka consumer configuration\n",
        "conf = {\n",
        "    \"bootstrap.servers\": \"pkc-6ojv2.us-west4.gcp.confluent.cloud:9092\",\n",
        "    \"security.protocol\": \"SASL_SSL\",\n",
        "    \"sasl.mechanisms\": \"PLAIN\",\n",
        "    \"sasl.username\": \"RFU2BCEDGB2UJRWD\",\n",
        "    \"sasl.password\": \"rjOrdR009uQUx8F67ObMn6on0IAgEtSunMFIKR2KK17SVJqQ1gh+9CmTT9NXiAjp\",\n",
        "    \"group.id\": \"my-consumer-group\",\n",
        "    \"auto.offset.reset\": \"earliest\"\n",
        "}\n",
        "\n",
        "# Define the Kafka topic to read from\n",
        "topic_in = 'KafkaTopicIN'\n",
        "topic_out = 'KafkaTopicOUT'\n",
        "\n",
        "# Define the schema for the messages\n",
        "schema = StructType([\n",
        "    StructField(\"availability\", StringType(), True),\n",
        "    StructField(\"brand\", StringType(), True),\n",
        "    StructField(\"category_id\", StringType(), True),\n",
        "    StructField(\"date\", StringType(), True),\n",
        "    StructField(\"price_height\", FloatType(), True),\n",
        "    StructField(\"price_low\", StringType(), True),\n",
        "    StructField(\"product_url\", StringType(), True),\n",
        "    StructField(\"sku\", StringType(), True),\n",
        "    StructField(\"store_id\", StringType(), True),\n",
        "    StructField(\"title\", StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "zpYTMsizx5Qp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kafka import KafkaConsumer, KafkaProducer\n",
        "from pymongo import MongoClient\n",
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
        "import json\n",
        "\n",
        "# Kafka consumer configuration\n",
        "bootstrap_servers = \"pkc-6ojv2.us-west4.gcp.confluent.cloud:9092\"\n",
        "security_protocol = \"SASL_SSL\"\n",
        "sasl_mechanism = \"PLAIN\"\n",
        "sasl_plain_username = \"RFU2BCEDGB2UJRWD\"\n",
        "sasl_plain_password = \"rjOrdR009uQUx8F67ObMn6on0IAgEtSunMFIKR2KK17SVJqQ1gh+9CmTT9NXiAjp\"\n",
        "group_id = \"my-consumer-group\"\n",
        "auto_offset_reset = \"earliest\"\n",
        "\n",
        "# Define the Kafka topic to read from\n",
        "topic_in = 'KafkaTopicIN'\n",
        "topic_out = 'KafkaTopicOUT'\n",
        "\n",
        "# Define the schema for the messages\n",
        "schema = StructType([\n",
        "    StructField(\"availability\", StringType(), True),\n",
        "    StructField(\"brand\", StringType(), True),\n",
        "    StructField(\"category_id\", StringType(), True),\n",
        "    StructField(\"date\", StringType(), True),\n",
        "    StructField(\"price_height\", FloatType(), True),\n",
        "    StructField(\"price_low\", StringType(), True),\n",
        "    StructField(\"product_url\", StringType(), True),\n",
        "    StructField(\"sku\", StringType(), True),\n",
        "    StructField(\"store_id\", StringType(), True),\n",
        "    StructField(\"title\", StringType(), True)\n",
        "])\n",
        "\n",
        "# MongoDB configuration parameters\n",
        "mongodb_uri = \"mongodb+srv://scrapper:shaylin1396*M@cluster0.cco0jwv.mongodb.net/?retryWrites=true&w=majority\"\n",
        "mongodb_database = \"deals\"\n",
        "mongodb_collection = \"good_deals\"\n",
        "\n",
        "# Window duration in seconds for each timeframe\n",
        "window_week = timedelta(weeks=1).total_seconds()\n",
        "window_month = timedelta(days=30).total_seconds()\n",
        "window_year = timedelta(days=365).total_seconds()\n",
        "window_historical = float(\"inf\")\n",
        "\n",
        "# Create a Kafka consumer with the provided configuration\n",
        "consumer = KafkaConsumer(\n",
        "    topic_in,\n",
        "    bootstrap_servers=bootstrap_servers,\n",
        "    security_protocol=security_protocol,\n",
        "    sasl_mechanism=sasl_mechanism,\n",
        "    sasl_plain_username=sasl_plain_username,\n",
        "    sasl_plain_password=sasl_plain_password,\n",
        "    group_id=group_id,\n",
        "    auto_offset_reset=auto_offset_reset\n",
        ")\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"KafkaWindowAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create a MongoDB client and connect to the database and collection\n",
        "client = MongoClient(mongodb_uri)\n",
        "db = client[mongodb_database]\n",
        "collection = db[mongodb_collection]\n",
        "\n",
        "# Create the database and collection if they don't exist\n",
        "if mongodb_database not in client.list_database_names():\n",
        "    db = client[mongodb_database]\n",
        "    collection = db.create_collection(mongodb_collection)\n",
        "\n",
        "# Process messages from Kafka\n",
        "for message in consumer:\n",
        "    # Decode the message value\n",
        "    message_value = message.value.decode('utf-8')\n",
        "\n",
        "    # Parse the JSON message\n",
        "    json_data = json.loads(message_value)\n",
        "\n",
        "    # Create a DataFrame from the JSON message\n",
        "    df = spark.createDataFrame([json_data], schema)\n",
        "\n",
        "    # Convert the date column to timestamp\n",
        "    df = df.withColumn(\"date\", to_timestamp(df[\"date\"]))\n",
        "\n",
        "    # Define the time-based window specification\n",
        "    windowSpec = Window.orderBy(\"date\").rowsBetween(-window_historical, 0)\n",
        "\n",
        "    # Add columns for each timeframe to indicate whether the price is good\n",
        "    df = df.withColumn(\"is_good_price_week\", df[\"price_height\"].between(\n",
        "        min(df[\"price_height\"]).over(windowSpec),\n",
        "        max(df[\"price_height\"]).over(windowSpec)\n",
        "    ))\n",
        "    df = df.withColumn(\"is_good_price_month\", df[\"price_height\"].between(\n",
        "        min(df[\"price_height\"]).over(windowSpec),\n",
        "        max(df[\"price_height\"]).over(windowSpec)\n",
        "    ))\n",
        "    df = df.withColumn(\"is_good_price_year\", df[\"price_height\"].between(\n",
        "        min(df[\"price_height\"]).over(windowSpec),\n",
        "        max(df[\"price_height\"]).over(windowSpec)\n",
        "    ))\n",
        "    df = df.withColumn(\"is_good_price_historical\", df[\"price_height\"].between(\n",
        "        min(df[\"price_height\"]).over(windowSpec),\n",
        "        max(df[\"price_height\"]).over(windowSpec)\n",
        "    ))\n",
        "\n",
        "    # Extract the required fields and convert to JSON\n",
        "    df = df.select(\n",
        "        \"product_url\", \"brand\", \"title\", \"price_height\",\n",
        "        \"is_good_price_week\", \"is_good_price_month\",\n",
        "        \"is_good_price_year\", \"is_good_price_historical\"\n",
        "    )\n",
        "    json_data = df.toJSON().first()\n",
        "\n",
        "    # Store the data in MongoDB\n",
        "    collection.insert_one(json.loads(json_data))\n",
        "\n",
        "    # Publish the good deals to Kafka\n",
        "    #producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
        "    #producer.send(topic_out, json_data.encode('utf-8'))\n",
        "    #producer.flush()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKIR6ujQyUd_",
        "outputId": "9648bdf7-569a-40d7-9b99-334ffbe9e288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:kafka.coordinator:Heartbeat failed for group my-consumer-group because it is rebalancing\n",
            "WARNING:kafka.coordinator:Heartbeat failed for group my-consumer-group because it is rebalancing\n",
            "WARNING:kafka.coordinator:Heartbeat failed for group my-consumer-group because it is rebalancing\n",
            "WARNING:kafka.coordinator:Heartbeat failed for group my-consumer-group because it is rebalancing\n",
            "WARNING:kafka.coordinator:Heartbeat failed for group my-consumer-group because it is rebalancing\n",
            "WARNING:kafka.coordinator:Heartbeat failed for group my-consumer-group because it is rebalancing\n",
            "WARNING:kafka.coordinator:Heartbeat failed for group my-consumer-group because it is rebalancing\n",
            "WARNING:kafka.coordinator:Heartbeat failed for group my-consumer-group because it is rebalancing\n",
            "WARNING:kafka.coordinator:Heartbeat failed for group my-consumer-group because it is rebalancing\n",
            "WARNING:kafka.coordinator:Heartbeat failed for group my-consumer-group because it is rebalancing\n",
            "WARNING:kafka.coordinator:Heartbeat failed for group my-consumer-group because it is rebalancing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-bJzY-GPyaIT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rb-KrE9JyvFm"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}